{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 3 Analyse how the hyper-parameter λ plays a role in deciding between bias and variance.\n",
    "\n",
    "* Large lamda ---> High Bias (Underfit)\n",
    "* Small lamda ---> High Variance (Overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que-4 Analyse how the two different regularisation techniques affect regression weights in terms of their values and what are the differences between the two.\n",
    "\n",
    "### Ridge Regression (L2)\n",
    "\n",
    "* It performs ‘L2 regularization’, i.e. adds penalty equivalent to square of the magnitude of coefficients. Thus, it optimises the following:\n",
    "\n",
    "#### Objective = RSS + lamda * (sum of square of coefficients)\n",
    "\n",
    "### lamda = 0:\n",
    "\n",
    "* The objective becomes same as simple linear regression.\n",
    "\n",
    "### lamda = ∞:\n",
    "\n",
    "* The coefficients will be zero.(Ideally)\n",
    "* Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite. It is the ideal case but in ridge regression, for large value of lamda it reduces weights of a feature nearest to zero but not exaclty zero.\n",
    "\n",
    "### 0 < lamda < ∞:\n",
    "* The magnitude of lamda will decide the weightage given to different parts of objective.\n",
    "* The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "\n",
    "## Lasso Regression (L1)\n",
    "\n",
    "* Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimisation objective.\n",
    "\n",
    "#### Objective = RSS + lamda * (sum of absolute value of coefficients)\n",
    "* Here, lamda works similar to that of ridge. Like that of ridge, lamda can take various values and provide a trade-off between balancing RSS and magnitude of coefficients.\n",
    "* If there are irrelevant features, Lasso is likely to make their weights zero.\n",
    "\n",
    "## Key Difference between Lasso and Ridge Regression\n",
    "\n",
    "* Ridge regression can’t zero coefficients. Here, you either select all the coefficients or none of them whereas Lasso does both parameter shrinkage and variable selection automatically because it zero out the co-efficients of collinear variables. Here it helps to select the variables out of given n variables while performing lasso regression.\n",
    "* Lasso is computationally expensive then ridge regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
